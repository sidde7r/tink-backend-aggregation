# EXPERIMENTAL APPROACH ALERTS V. 2.0
#
# When a time series appears for the first time, event associated with its creation
# is missed by increase() since we don't have a reference point (no observable increase).
# This is due to metrics not being zero-initialized.
# Assuming query interval of one hour, the approach makes use of the following heuristics:
# if a time series is "old", that is appeared more than an hour ago, increase() behaves correctly.
# However, if a time series has appeared more recently, we want to be able to count the first event,
# thus adding 1 to the result. Effectively, we want to calculate:
#
# if (time series is old) then increase(...) else increase(...) + 1
#
# Determining if a time series is "old" or not is done in an extremely hacky fashion
# of counting a number of target scrapes within the last hour.
# A series is considered to be "old" if and only if it contains at least
# <query interval> / <scrape interval> = 1h / 10s = 360 data points
# (361 is possible to due to off-by-one errors and scrape latencies)
#
# The expression above is equivalent to
#   1 + increase(...) - (if "old" then 1 else 0)
#   =
#   1 + increase(...) - floor(<number_of_data_points> / 360)
#
# floor(increase(...)) is used instead of raw increase(...) to account for
# its undesirable extrapolation behavior and floating-point inaccuracies.
#
# One corner case where this approach fails: when a time series appeared more than
# <query interval> ago, but there was a counter reset in the meantime.
# This is an event rare enough not to significantly affect the end result.

kind: RuleGroup
apiVersion: monitoring.tink.se/v1
metadata:
  name: alerts-gb-experimental
  namespace: agents
spec:
  rules:
    - record: sum:all_logins_gb:delta1h
      expr: |
        sum by (className) (
          1 + floor(increase(tink_agent_login_total{action="login", market="GB", provider_type!="test"}[1h]))
          -
          floor(count_over_time(tink_agent_login_total{action="login", market="GB", provider_type!="test"}[1h]) / 360)
        )
    - record: sum:all_logins_gb:delta4h
      expr: |
        sum by (className) (
          1 + floor(increase(tink_agent_login_total{action="login", market="GB", provider_type!="test"}[4h]))
          -
          floor(count_over_time(tink_agent_login_total{action="login", market="GB", provider_type!="test"}[4h]) / 1440)
        )
    - record: sum:all_refreshes_per_type_gb:delta1h
      expr: |
        sum by (className, type) (
          1 + floor(increase(tink_agent_refresh_total{action="refresh", market="GB", provider_type!="test"}[1h]))
          -
          floor(count_over_time(tink_agent_refresh_total{action="refresh", market="GB", provider_type!="test"}[1h]) / 360)
        )
    - record: sum:failed_logins_gb:delta1h
      expr: |
        sum by (className) (
          1 + floor(increase(tink_agent_login_total{action="login", outcome=~"failed.*", market="GB", provider_type!="test"}[1h]))
          -
          floor(count_over_time(tink_agent_login_total{action="login", outcome=~"failed.*", market="GB", provider_type!="test"}[1h]) / 360)
        )
    - record: sum:unsuccessful_background_logins_gb:delta4h
      expr: |
        sum by (className) (
          1 + floor(increase(tink_agent_login_total{action="login-cron", outcome!="completed", market="GB", provider_type!="test"}[4h]))
          -
          floor(count_over_time(tink_agent_login_total{action="login-cron", outcome!="completed", market="GB", provider_type!="test"}[4h]) / 1440)
        )
    - record: sum:unsuccessful_on_demand_logins_gb:delta4h
      expr: |
        sum by (className) (
          1 + floor(increase(tink_agent_login_total{action=~"login-manual|login-auto", outcome!="completed", market="GB", provider_type!="test"}[4h]))
          -
          floor(count_over_time(tink_agent_login_total{action=~"login-manual|login-auto", outcome!="completed", market="GB", provider_type!="test"}[4h]) / 1440)
        )
    - record: sum:failed_refreshes_per_type_gb:delta1h
      expr: |
        sum by (className, type) (
          1 + floor(increase(tink_agent_refresh_total{action="refresh", outcome=~"failed.*|partially_completed", market="GB", provider_type!="test"}[1h]))
          -
          floor(count_over_time(tink_agent_refresh_total{action="refresh", outcome=~"failed.*|partially_completed", market="GB", provider_type!="test"}[1h]) / 360)
        )

    # =========== SHORT-TERM (1h) and MID-TERM (4h) ALERTS ===========
    - alert: EXPERIMENTALRefreshErrorRateAbove20%GB
      expr: sum:failed_refreshes_per_type_gb:delta1h / (sum:all_refreshes_per_type_gb:delta1h > 5) > 0.2
      for: 15m
      priority: P3
      labels:
        severity: ping
        who: integration-fluffy
      annotations:
        description: {{ .Values.kibanaClassLinkAbs1h }}
          {{ .Values.grafanaClassLinkAbs1h }}
        summary: Refresh failures for <{ $labels.type }> over the last hour is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>

    - alert: EXPERIMENTALRefreshErrorRateAbove90%GB
      expr: sum:failed_refreshes_per_type_gb:delta1h / (sum:all_refreshes_per_type_gb:delta1h > 2) > 0.9
      for: 15m
      priority: P1
      labels:
        severity: urgent
        who: integration-fluffy
      annotations:
        description: {{ .Values.kibanaClassLinkAbs1h }}
          {{ .Values.grafanaClassLinkAbs1h }}
        summary: Refresh failures for <{ $labels.type }> over the last hour is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>

    - alert: EXPERIMENTALLoginErrorRateAbove10%GB
      expr: sum:failed_logins_gb:delta1h / sum:all_logins_gb:delta1h > 0.1
      for: 15m
      priority: P3
      labels:
        severity: ping
        who: integration-fluffy
      annotations:
        description: {{ .Values.kibanaClassLinkAbs1h }}
          {{ .Values.grafanaClassLinkAbs1h }}
        summary: Login error rate over the last hour is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>

    - alert: EXPERIMENTALLoginErrorRateAbove90%GB
      expr: sum:failed_logins_gb:delta1h / (sum:all_logins_gb:delta1h > 2) > 0.9
      for: 15m
      priority: P1
      labels:
        severity: urgent
        who: integration-fluffy
      annotations:
        description: {{ .Values.kibanaClassLinkAbs1h }}
          {{ .Values.grafanaClassLinkAbs1h }}
        summary: Login error rate over the last hour is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>

    - alert: EXPERIMENTALBackgroundLoginErrorRateAbove90%GB
      expr: sum:unsuccessful_background_logins_gb:delta4h / (sum:all_logins_gb:delta4h > 5) > 0.9
      for: 15m
      priority: P2
      labels:
        severity: urgent
        who: integration-fluffy
      annotations:
        description: {{ .Values.kibanaClassLinkAbs4h }}
          {{ .Values.grafanaClassLinkAbs4h }}
        summary: Background login error rate over the last 4 hours is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>

    - alert: EXPERIMENTALOnDemandLoginErrorRateAbove90%GB
      expr: sum:unsuccessful_on_demand_logins_gb:delta4h / (sum:all_logins_gb:delta4h > 5) > 0.9
      for: 15m
      priority: P2
      labels:
        severity: urgent
        who: integration-fluffy
      annotations:
        description: {{ .Values.kibanaClassLinkAbs4h }}
          {{ .Values.grafanaClassLinkAbs4h }}
        summary: On-demand login error rate over the last 4 hours is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>
