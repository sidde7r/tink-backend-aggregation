# EXPERIMENTAL APPROACH ALERTS V. 3.0
# Assuming that counter resets are rare, that is for any query interval there has been
# at most one counter reset within that interval, there are five distinct cases to consider.
# Assume for the sake of an example that the query interval is one hour.
# '[' and ']' denote the beginning and end of the query interval, respectively.
# 'X' denotes counter reset.

# Case A: no counter resets within the last hour:
#
#       now-1h                                          now
#         |                                              |
# --------[----------------------------------------------]
#
# Case B: counter reset and reappearance of a time series in the meantime:
# --------[----------------X        N--------------------]
#
# Case C: counter reset and time series not appearing again after
# --------[-----------------------------X                ]
#
# Case D: counter reset a long time ago and time series reappearing in the meantime:
# ---X    [                  N---------------------------]
#
# Case E: as above but with counter reset
# ---X   [          N------------------X                 ]
#
# Let T be the query selector we're interested in, for instance
#   tink_agent_login_total{action="login", market="DK", provider_type!="test"}
#
# Correct result is increase(T[1h]) in cases A and C, and increase(T[1h]) + 1 in cases B,D and E
# (because we want to account for new events - denoted by 'N' - which increase() alone would not account for)
#
# Let Δ = increase(T[1h])
#
# Step-by-step explanation of the expression calculating the *actual* number of events
# that have occurred within last hour:
#
# First, we calculate Δ for each time series and keep only those
# whose counter value was scraped an hour ago, but not ones whose value
# was scraped at the time of query (in other words: keep only case C):

# Δ and ((T offset 1h) unless T)        % `unless` is set difference
#
# Then, we calculate Δ+1 for every time series, which will leave us with correct result for
# every remaining case except case A.
# Let X = floor(count_over_time(T[1h]) / 360)
# X is equal to 1 in case A (no counter resets) and 0 in cases B, C and D, E
# Since case A covers the full query interval,
# it will have at least 1h / <scrape_interval> = 1h / 10s = 360 scraped data points
# (361 is also possible).
# To attain the desired result, we subtract 1 from case A, or equivalently,
# subtract X from every time series.
#
# Finally, the correct result is a set union of these two:
#
# Δ and ((T offset 1h) unless T)
# or
# 1 + Δ - X
#
# `or` is left-biased, so it will keep the result for case C calculated in step 1 and
# complement it with results for the remaining cases calculated in step 2).

# floor(increase(...)) is used instead of raw increase(...)
# to account for its undesirable extrapolation behavior.
#
# Due to scrape latencies and other unavoidable artifacts, it's not possible to achieve 100% accuracy
# without zero-initializing metrics. Nevertheless, this approach provides results
# with a very small relative difference from the actual number of events (as reported by Kibana).
#
# TODO: provide an in-depth explanation of the approach as a Confluence page (with pictures!)

kind: RuleGroup
apiVersion: monitoring.tink.se/v1
metadata:
  name: alerts-dk
  namespace: agents
spec:
  rules:
    - record: increase:all_logins_dk:delta1h
      expr: floor(increase(tink_agent_login_total{action="login", market="DK", provider_type!="test"}[1h]))
    - record: sum:all_logins_dk:delta1h
      expr: |
        sum by (className) (
          (increase:all_logins_dk:delta1h
           and
           (tink_agent_login_total{action="login", market="DK", provider_type!="test"} offset 1h
            unless tink_agent_login_total{action="login", market="DK", provider_type!="test"}))
          or
          (1 + increase:all_logins_dk:delta1h - floor(count_over_time(tink_agent_login_total{action="login", market="DK", provider_type!="test"}[1h]) / 360))
        )

    # ALL LOGIN ATTEMPTS (4h)
    - record: increase:all_logins_dk:delta4h
      expr: floor(increase(tink_agent_login_total{action="login", market="DK", provider_type!="test"}[4h]))
    - record: sum:all_logins_dk:delta4h
      expr: |
        sum by (className) (
          (increase:all_logins_dk:delta4h
           and
           (tink_agent_login_total{action="login", market="DK", provider_type!="test"} offset 4h
            unless tink_agent_login_total{action="login", market="DK", provider_type!="test"}))
          or
          (1 + increase:all_logins_dk:delta4h - floor(count_over_time(tink_agent_login_total{action="login", market="DK", provider_type!="test"}[4h]) / 1440))
        )

    # FAILED LOGIN ATTEMPTS (1h)
    - record: increase:failed_logins_dk:delta1h
      expr: floor(increase(tink_agent_login_total{action="login", outcome=~"failed.*", outcome=~"failed.*", market="DK", provider_type!="test"}[1h]))
    - record: sum:failed_logins_dk:delta1h
      expr: |
        sum by (className) (
          (increase:failed_logins_dk:delta1h
           and
           (tink_agent_login_total{action="login", outcome=~"failed.*", outcome=~"failed.*", market="DK", provider_type!="test"} offset 1h
            unless tink_agent_login_total{action="login", outcome=~"failed.*", outcome=~"failed.*", market="DK", provider_type!="test"}))
          or
          (1 + increase:failed_logins_dk:delta1h - floor(count_over_time(tink_agent_login_total{action="login", outcome=~"failed.*", outcome=~"failed.*", market="DK", provider_type!="test"}[1h]) / 360))
        )

    # UNSUCCESSFUL BACKGROUND LOGINS (4h)
    - record: increase:unsuccessful_background_logins_dk:delta4h
      expr: floor(increase(tink_agent_login_total{action="login-cron", outcome!="completed", market="DK", provider_type!="test"}[4h]))
    - record: sum:unsuccessful_background_logins_dk:delta4h
      expr: |
        sum by (className) (
          (increase:unsuccessful_background_logins_dk:delta4h
           and
           (tink_agent_login_total{action="login-cron", outcome!="completed", market="DK", provider_type!="test"} offset 4h
            unless tink_agent_login_total{action="login-cron", outcome!="completed", market="DK", provider_type!="test"}))
          or
          (1 + increase:unsuccessful_background_logins_dk:delta4h - floor(count_over_time(tink_agent_login_total{action="login-cron", outcome!="completed", market="DK", provider_type!="test"}[4h]) / 1440))
        )

    # UNSUCCESSFUL ON-DEMAND LOGINS (4h)
    - record: increase:unsuccessful_on_demand_logins_dk:delta4h
      expr: floor(increase(tink_agent_login_total{action=~"login-manual|login-auto", outcome!="completed", market="DK", provider_type!="test"}[4h]))
    - record: sum:unsuccessful_on_demand_logins_dk:delta4h
      expr: |
        sum by (className) (
          (increase:unsuccessful_on_demand_logins_dk:delta4h
           and
           (tink_agent_login_total{action=~"login-manual|login-auto", outcome!="completed", market="DK", provider_type!="test"} offset 4h
            unless tink_agent_login_total{action=~"login-manual|login-auto", outcome!="completed", market="DK", provider_type!="test"}))
          or
          (1 + increase:unsuccessful_on_demand_logins_dk:delta4h - floor(count_over_time(tink_agent_login_total{action=~"login-manual|login-auto", outcome!="completed", market="DK", provider_type!="test"}[4h]) / 1440))
        )

    # ALL REFRESH ATTEMPTS PER TYPE (1h)
    - record: increase:all_refreshes_per_type_dk:delta1h
      expr: floor(increase(tink_agent_refresh_total{action="refresh", market="DK", provider_type!="test"}[1h]))
    - record: sum:all_refreshes_per_type_dk:delta1h
      expr: |
        sum by (className, type) (
          (increase:all_refreshes_per_type_dk:delta1h
           and
           (tink_agent_refresh_total{action="refresh", market="DK", provider_type!="test"} offset 1h
            unless tink_agent_refresh_total{action="refresh", market="DK", provider_type!="test"}))
          or
          (1 + increase:all_refreshes_per_type_dk:delta1h - floor(count_over_time(tink_agent_refresh_total{action="refresh", market="DK", provider_type!="test"}[1h]) / 360))
        )

    # FAILED REFRESH ATTEMPTS PER TYPE (1h)
    - record: increase:failed_refreshes_per_type_dk:delta1h
      expr: floor(increase(tink_agent_refresh_total{action="refresh", outcome=~"failed.*|partially_completed", market="DK", provider_type!="test"}[1h]))
    - record: sum:failed_refreshes_per_type_dk:delta1h
      expr: |
        sum by (className, type) (
          (increase:failed_refreshes_per_type_dk:delta1h
           and
           (tink_agent_refresh_total{action="refresh", outcome=~"failed.*|partially_completed", market="DK", provider_type!="test"} offset 1h
            unless tink_agent_refresh_total{action="refresh", outcome=~"failed.*|partially_completed", market="DK", provider_type!="test"}))
          or
          (1 + increase:failed_refreshes_per_type_dk:delta1h - floor(count_over_time(tink_agent_refresh_total{action="refresh", outcome=~"failed.*|partially_completed", market="DK", provider_type!="test"}[1h]) / 360))
        )

    # =========== SHORT-TERM (1h) and MID-TERM (4h) ALERTS ===========
    - alert: RefreshErrorRateAbove20%DK
      # Avoid alert duplication caused by the rule below this one
      expr: 0.2 <= sum:failed_refreshes_per_type_dk:delta1h / (sum:all_refreshes_per_type_dk:delta1h >= 5) <= 0.9
      for: 15m
      priority: P3
      labels:
        severity: ping
        who: integration-teletubbies
      annotations:
        description: {{ .Values.kibanaClassLinkAbs1h }}
          {{ .Values.grafanaClassLinkAbs1h }}
        summary: Refresh failures for <{ $labels.type }> over the last hour is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>

    - alert: RefreshErrorRateAbove90%DK
      expr: sum:failed_refreshes_per_type_dk:delta1h / (sum:all_refreshes_per_type_dk:delta1h >= 5) > 0.9
      for: 15m
      priority: P1
      labels:
        severity: urgent
        who: integration-teletubbies
      annotations:
        description: {{ .Values.kibanaClassLinkAbs1h }}
          {{ .Values.grafanaClassLinkAbs1h }}
        summary: Refresh failures for <{ $labels.type }> over the last hour is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>

    - alert: LoginErrorRateAbove20%DK
      expr: 0.2 <= sum:failed_logins_dk:delta1h / (sum:all_logins_dk:delta1h >= 5) <= 0.9
      for: 15m
      priority: P3
      labels:
        severity: ping
        who: integration-teletubbies
      annotations:
        description: {{ .Values.kibanaClassLinkAbs1h }}
          {{ .Values.grafanaClassLinkAbs1h }}
        summary: Login error rate over the last hour is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>

    - alert: LoginErrorRateAbove90%DK
      expr: sum:failed_logins_dk:delta1h / (sum:all_logins_dk:delta1h >= 5) > 0.9
      for: 15m
      priority: P1
      labels:
        severity: urgent
        who: integration-teletubbies
      annotations:
        description: {{ .Values.kibanaClassLinkAbs1h }}
          {{ .Values.grafanaClassLinkAbs1h }}
        summary: Login error rate over the last hour is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>

    - alert: BackgroundLoginErrorRateAbove90%DK
      expr: sum:unsuccessful_background_logins_dk:delta4h / (sum:all_logins_dk:delta4h > 5) > 0.9
      for: 15m
      priority: P2
      labels:
        severity: urgent
        who: integration-teletubbies
      annotations:
        description: {{ .Values.kibanaClassLinkAbs4h }}
          {{ .Values.grafanaClassLinkAbs4h }}
        summary: Background login error rate over the last 4 hours is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>

    - alert: OnDemandLoginErrorRateAbove90%DK
      expr: sum:unsuccessful_on_demand_logins_dk:delta4h / (sum:all_logins_dk:delta4h > 5) > 0.9
      for: 15m
      priority: P2
      labels:
        severity: urgent
        who: integration-teletubbies
      annotations:
        description: {{ .Values.kibanaClassLinkAbs4h }}
          {{ .Values.grafanaClassLinkAbs4h }}
        summary: On-demand login error rate over the last 4 hours is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: <{ $labels.className }>
