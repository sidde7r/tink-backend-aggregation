# EXPERIMENTAL APPROACH ALERTS V. 2.0
#
# Explanation:
# When a time series appears for the first time, event associated with its creation
# is missed by increase() since we don't have a reference point (no observable increase).
# This is due to metrics not being zero-initialized.
# The approach makes use of the following heuristics: if a time series is "old", that is
# appeared more than an hour ago, increase() behaves correctly.
# However, if a time series has appeared more recently, we want to be able to count the first event,
# thus adding 1 to the result. Effectively, we want to calculate:
#
# if (time series is old) then increase(...) else increase(...) + 1
#
# Determining if a time series is "old" or not is done in an extremely hacky fashion
# of counting a number of target scrapes within the last hour.
# A series is considered to be "old" if and only if it contains at least
# <query interval> / <scrape interval> = 1h / 10s = 360 data points
# (361 is possible to due to off-by-one errors and scrape latencies)
#
# The expression above is equivalent to
#   1 + increase(...) - (if "old" then 1 else 0)
#   =
#   1 + increase(...) - floor(<number_of_data_points> / 360)
#
# floor(increase(...)) is used instead of raw increase(...) to account for
# its undesirable extrapolation behavior and floating-point inaccuracies
#
# All this operates under the assumption that counter resets are rare
# (that is there is at most one counter reset within the last hour)
# which is reasonable for short-term queries, but makes this approach
# infeasible for long-term alerts.

kind: RuleGroup
apiVersion: monitoring.tink.se/v1
metadata:
  name: alerts-dk-experimental
  namespace: agents
spec:
  interval: 5m
  rules:
    - record: sum:dk_login_all_attempts:delta1h
      expr: |
        sum by (className, action) (
          1 + floor(increase(tink_agent_login_total{market="DK", provider_type!="test"}[1h]))
          -
          floor(count_over_time(tink_agent_login_total{market="DK", provider_type!="test"}[1h]) / 360)
        )
    - record: sum:dk_login_all_errors:delta1h
      expr: |
        sum by (className, action) (
          1 + floor(increase(tink_agent_login_total{market="DK", outcome!="completed", provider_type!="test"}[1h]))
          -
          floor(count_over_time(tink_agent_login_total{market="DK", outcome!="completed", provider_type!="test"}[1h]) / 360)
        )
    - record: sum:dk_login_unavailable:delta1h
      expr: |
        sum by (className, action) (
          1 + floor(increase(tink_agent_login_total{market="DK", outcome="unavailable", provider_type!="test"}[1h]))
          -
          floor(count_over_time(tink_agent_login_total{market="DK", outcome="unavailable", provider_type!="test"}[1h]) / 360)
        )
    - record: sum:dk_login_temporary_errors:delta1h
      expr: |
        sum by (className, action) (
          1 + floor(increase(tink_agent_login_total{market="DK", outcome=~"failed.*", provider_type!="test"}[1h]))
          -
          floor(count_over_time(tink_agent_login_total{market="DK", outcome=~"failed.*", provider_type!="test"}[1h]) / 360)
        )

    - alert: LoginUnavailableErrorRateHigherThan20%DK
      expr: sum:dk_login_unavailable:delta1h / (sum:dk_login_all_attempts:delta1h > 5) > 0.2
      for: 15m
      priority: P4
      labels:
        severity: ping
        who: integration-teletubbies
      annotations:
        description: {{ .Values.kibanaClassLinkAbs1h }} {{ .Values.grafanaClassLinkAbs1h }}
        summary: Login "unavailable" error rate over the last 1 hour is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: {{ .Values.tags }}
    - alert: LoginTemporaryErrorsRateHigherThan10%DK
      expr: sum:dk_login_temporary_errors:delta1h / (sum:dk_login_all_attempts:delta1h > 5) > 0.2
      for: 15m
      priority: P4
      labels:
        severity: ping
        who: integration-teletubbies
      annotations:
        description: {{ .Values.kibanaClassLinkAbs1h }} {{ .Values.grafanaClassLinkAbs1h }}
        summary: Login temporary errors rate over the last 1 hour is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: {{ .Values.tags }}
    - alert: LoginAllErrorsRateHigherThan90%DK
      expr: sum:dk_login_all_errors:delta1h / (sum:dk_login_all_attempts:delta1h > 5) > 0.9
      for: 15m
      priority: P4
      labels:
        severity: ping
        who: integration-teletubbies
      annotations:
        description: {{ .Values.kibanaClassLinkAbs1h }} {{ .Values.grafanaClassLinkAbs1h }}
        summary: Login error rate over the last 1 hour is <{ $value | humanizePercentage }> for <{ $labels.className }>
        tags: {{ .Values.tags }}
